{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Retrain an MLP using a SHAP feature list and Optuna best_params.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from joblib import dump\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler, FunctionTransformer, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# ================== PATHS / CONFIG ==================\n",
        "DATA_PATH = Path(\"dataset_enriched_filtered.xlsx\")   # <-- adjust\n",
        "TARGET_COL = \"phase_disambiguated\"\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# ================== List of features selected by SHAP ==================\n",
        "SELECTED_FEATURES = [\n",
        "    \"critical_temperature_max\",\n",
        "    \"electrical_resist_sum\",\n",
        "    \"critical_temperature_mean\",\n",
        "    \"critical_temperature_sum\",\n",
        "    \"critical_temperature_stdv\",\n",
        "    \"electrical_resist_stdv\",\n",
        "    \"electrical_resist_mean\",\n",
        "    \"vickers_hardness_min\",\n",
        "    \"electrical_resist_max\",\n",
        "    \"atomic_en_sanderson_min_to_max_ratio\",\n",
        "    \"electrical_resist_range\",\n",
        "    \"atomic_hfu_std_over_range\",\n",
        "    \"gibbs-oxides_min_to_max_ratio\",\n",
        "    \"atomic_ea_min_to_max_ratio\",\n",
        "    \"rigidity_mod_min_to_max_ratio\",\n",
        "    \"deltacp-oxides_min_to_max_ratio\",\n",
        "    \"deltacp-oxides_cov\",\n",
        "    \"reflectivity_min\",\n",
        "    \"covalent_rad_emp_sum\",\n",
        "    \"poissons_ratio_min_to_max_ratio\",\n",
        "    \"enthalpy-oxides_max\",\n",
        "    \"thermal_conduct_min\",\n",
        "    \"covalent_rad_range\",\n",
        "    \"deltacp-oxides_min\",\n",
        "    \"atomic_ea_std_over_range\",\n",
        "    \"bulk_mod_min_to_max_ratio\",\n",
        "    \"gibbs-oxides_max\",\n",
        "    \"entropy-oxides_mean\",\n",
        "    \"atomic_enc_sum\",\n",
        "    \"covalent_rad_min_to_max_ratio\",\n",
        "    \"coeff_of_lte_std_over_range\",\n",
        "    \"atomic_orbital_radii_cov\",\n",
        "    \"valence_d_electrons_cov\",\n",
        "    \"has_zero_paul_ionic_radii_min\",\n",
        "    \"vec_stdv\",\n",
        "    \"atomic_en_sanderson_min\",\n",
        "    \"enthalpy-oxides_std_over_range\",\n",
        "    \"refract_index_stdv\",\n",
        "    \"covalent_rad_emp_stdv\",\n",
        "    \"oxidation_std_over_range\",\n",
        "    \"rigidity_mod_std_over_range\",\n",
        "    \"coeff_of_lte_min\",\n",
        "    \"vec_sum\",\n",
        "    \"covalent_rad_stdv\",\n",
        "    \"deltacp-oxides_range\",\n",
        "    \"z_std_over_range\",\n",
        "    \"atomic_enc_cov\",\n",
        "    \"atomic_spacegroupnum_std_over_range\",\n",
        "    \"rigidity_mod_range\",\n",
        "    \"deltacp-oxides_std_over_range\",\n",
        "    \"boiling_point_stdv\",\n",
        "    \"atomic_en_allen__max\",\n",
        "    \"atomic_hatm_mean\",\n",
        "    \"atomic_en_sanderson_max\",\n",
        "    \"atomic_en_allredroch_max\",\n",
        "    \"atomic_ebe__range\"\n",
        "]\n",
        "\n",
        "# ================== BEST PARAMS (from your Optuna run) ==================\n",
        "best_params = {\n",
        "    \"n_layers\": 1,\n",
        "    \"n_units_l1\": 24,\n",
        "    \"activation\": \"tanh\",\n",
        "    \"solver\": \"adam\",\n",
        "    \"alpha\": 0.0023404113539471617,\n",
        "    \"tol\": 1.2324561293087634e-05,\n",
        "    \"max_iter\": 30000,\n",
        "    \"shuffle\": True,\n",
        "    \"warm_start\": False,\n",
        "    \"batch_choice\": \"auto\",\n",
        "    \"learning_rate_init\": 0.001143876930032853,\n",
        "    \"beta_1\": 0.8891631637090696,\n",
        "    \"beta_2\": 0.999513887975822,\n",
        "    \"epsilon\": 1.6097066118907405e-10\n",
        "}\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1) Load dataset\n",
        "    if not DATA_PATH.exists():\n",
        "        raise FileNotFoundError(f\"Dataset not found: {DATA_PATH}\")\n",
        "\n",
        "    df = pd.read_excel(DATA_PATH)\n",
        "    if TARGET_COL not in df.columns:\n",
        "        raise KeyError(f\"Target column '{TARGET_COL}' not found in dataset. Available: {list(df.columns)[:10]} ...\")\n",
        "\n",
        "    df = df.dropna(subset=[TARGET_COL]).reset_index(drop=True)\n",
        "    y_raw = df[TARGET_COL]\n",
        "    X_full = df.drop(columns=[TARGET_COL]).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # 2) Ensure all SELECTED_FEATURES are present\n",
        "    missing = [c for c in SELECTED_FEATURES if c not in X_full.columns]\n",
        "    if missing:\n",
        "        raise KeyError(\n",
        "            \"O dataset não contém TODAS as features fixas requeridas.\\n\"\n",
        "            f\"Faltando ({len(missing)}): {missing[:10]}{' ...' if len(missing) > 10 else ''}\\n\"\n",
        "            \"Garanta que estas colunas existam antes de treinar.\"\n",
        "        )\n",
        "\n",
        "    # Keep only selected features in fixed order\n",
        "    X = X_full[SELECTED_FEATURES].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "    # 3) Split + Label encode\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_raw)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # 4) Preprocessing (NO LAMBDA): impute -> scale\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", Pipeline([\n",
        "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                (\"scaler\", RobustScaler()),\n",
        "            ]), SELECTED_FEATURES),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "        verbose_feature_names_out=False\n",
        "    )\n",
        "\n",
        "    # 5) MLP from best_params\n",
        "    n_layers = int(best_params.get(\"n_layers\", 1))\n",
        "    hidden_layer_sizes = tuple(int(best_params.get(f\"n_units_l{i+1}\", 100)) for i in range(n_layers))\n",
        "    batch_choice = best_params.get(\"batch_choice\", \"auto\")\n",
        "    batch_size = \"auto\" if (isinstance(batch_choice, str) and batch_choice.lower() == \"auto\") else int(batch_choice)\n",
        "\n",
        "    clf = MLPClassifier(\n",
        "        hidden_layer_sizes=hidden_layer_sizes,\n",
        "        activation=best_params.get(\"activation\", \"relu\"),\n",
        "        solver=best_params.get(\"solver\", \"adam\"),\n",
        "        alpha=float(best_params.get(\"alpha\", 1e-4)),\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=\"constant\",\n",
        "        learning_rate_init=float(best_params.get(\"learning_rate_init\", 1e-3)),\n",
        "        max_iter=int(best_params.get(\"max_iter\", 30000)),\n",
        "        shuffle=bool(best_params.get(\"shuffle\", True)),\n",
        "        random_state=RANDOM_STATE,\n",
        "        tol=float(best_params.get(\"tol\", 1e-4)),\n",
        "        warm_start=bool(best_params.get(\"warm_start\", False)),\n",
        "        early_stopping=False,\n",
        "        validation_fraction=0.1,\n",
        "        n_iter_no_change=10,\n",
        "        beta_1=float(best_params.get(\"beta_1\", 0.9)),\n",
        "        beta_2=float(best_params.get(\"beta_2\", 0.999)),\n",
        "        epsilon=float(best_params.get(\"epsilon\", 1e-8)),\n",
        "    )\n",
        "\n",
        "    pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
        "\n",
        "    # Fit\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    y_hat = pipe.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_hat)\n",
        "    f1m = f1_score(y_test, y_hat, average=\"macro\")\n",
        "    print(f\"Holdout Accuracy: {acc:.4f} | F1-macro: {f1m:.4f}\")\n",
        "    print(\"Classification report (label indices):\\n\", classification_report(y_test, y_hat))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfOkMzhYKanS",
        "outputId": "98148b2f-7581-44a3-c1ed-a52f141e2208"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Holdout Accuracy: 0.9790 | F1-macro: 0.9766\n",
            "Classification report (label indices):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        96\n",
            "           1       0.94      1.00      0.97        47\n",
            "\n",
            "    accuracy                           0.98       143\n",
            "   macro avg       0.97      0.98      0.98       143\n",
            "weighted avg       0.98      0.98      0.98       143\n",
            "\n"
          ]
        }
      ]
    }
  ]
}